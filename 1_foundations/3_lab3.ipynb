{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_API_KEY Key exists and begins AIzaSyCq\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"GOOGLE_API_KEY Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"GOOGLE_API_KEY not set - please head to the troubleshooting guide in the setup folder\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_BASE_URL=\"https://generativelanguage.googleapis.com/v1beta/openai/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL=\"http://localhost:11434/v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama=OpenAI(base_url=OLLAMA_BASE_URL,api_key=\"anything\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GEMINI_BASE_URL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m load_dotenv(override=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m gemini=OpenAI(base_url=\u001b[43mGEMINI_BASE_URL\u001b[49m,api_key=google_api_key)\n",
      "\u001b[31mNameError\u001b[39m: name 'GEMINI_BASE_URL' is not defined"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "gemini=OpenAI(base_url=GEMINI_BASE_URL,api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankit Kolhe\n",
      "Nagpur, Maharashtra· nkolhe149@gmail.com · 7666846804 · linkedin/ankit-kolhe\n",
      "Education\n",
      "Shri Ramdeobaba College of Engineering and Management Nagpur\n",
      "B.Tech. Computer Science(Artiﬁcial Intelligence And Machine Learning)GPA: 9.51 Aug 2023 - July 2027\n",
      "Shri Shivaji Science College Nagpur\n",
      "HSC GPA: 81% Aug 2020 - July 2022\n",
      "Bharti Krishna Vidya Vihar Nagpur\n",
      "SSC GPA: 98.6% April 2010 - May 2020\n",
      "Skills\n",
      "Programming Languages: C++, Python, JavaScript\n",
      "Framework and Libraries: React.js, Node.js, Django\n",
      "Databases: MS SQL Server, MongoDB, PostgreSQL\n",
      "DevOps & Tools: Git/Github, Docker\n",
      "Machine Learning: TensorFlow, Pytorch, Natural Language Processing, Fine-tuning LLM, RAG\n",
      "Projects\n",
      "Sumy-Sumarise Python, NLP, Streamlit, scikit-learn https://sumarise-nlp.streamlit.app/\n",
      "Built an NLP-based news summarization system using Latent Semantic Analysis (LSA) and sentiment\n",
      "analysis, reducing article length according to user's convenience while retaining >85% key information.\n",
      "Designed a complete pipeline (parsing, tokenization, stemming, LSA) that processed 100+ articles with\n",
      "remarkable accuarcy.\n",
      "DrowsyMe Python, MongoDB, Streamlit, scikit-learn https://drowsyme.streamlit.app/\n",
      "Developed a real-time drowsiness and gaze tracking web app using Streamlit, OpenCV, dlib, and MediaPipe,\n",
      "enabling live EAR-based detection and gaze monitoring (center/left/right/down). Built user/admin\n",
      "dashboards to log and analyze 10+ session metrics, improving exam/meeting monitoring eﬃciency by 60%\n",
      "with automated alerts and detailed attention reports.\n",
      "Real-Time Cryptocurrency ETL PipelinePython, Kafka, Docker, MongoDB\n",
      "https://github.com/AnkitKolhe149/data_engineering_2025\n",
      "Applied Pipeline Engineering to design a real-time data pipeline using Python, Apache Kafka, and Docker to\n",
      "monitor cryptocurrency prices. Conﬁgured a local distributed environment to ingest live data from the\n",
      "CoinGecko API, utilizing a decoupled producer-consumer architecture. Successfully processed and stored\n",
      "ﬁnancial metrics into MongoDB, demonstrating hands-on expertise in message queuing, containerization, and\n",
      "NoSQL database integration.\n",
      "Work Experience\n",
      "Technical Club , RBU Nagpur\n",
      "3D Modelling And Animation Lead Nov 2024 - Present\n",
      "• Collaboration and Coordination\n",
      "• Timely Delivery of Content\n",
      "Awards and Certifications\n",
      "AWS Cloud Foundations AWS Academy\n",
      "Tools: EC2, IAM, Lambda, S3 June 2025\n",
      "AWS Machine Learning Foundations AWS Academy\n",
      "Tools - SageMaker, Rekognition, Comprehend, Polly May 2025\n",
      "Introduction to Machine Learning NPTEL\n",
      "Supervised, Unsupervised and Reinforcement Learning Nov 2025\n",
      "Introduction to Large Language Models NPTEL\n",
      "RAG, Pre-trained Model Optimization, NLP, Embedding, Decoding Nov 2025\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Ankit Kolhe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Ankit Kolhe. You are answering questions on Ankit Kolhe's website, particularly questions related to Ankit Kolhe's career, background, skills and experience. Your responsibility is to represent Ankit Kolhe for interactions on the website as faithfully as possible. You are given a summary of Ankit Kolhe's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Ankit Kolhe. I'm an diligent student and an aspiring software engineer and data scientist. I'm originally from Nagpur, India.\\nI  am an INTJ, and I like all kinds of things except noisy stuff and stuff that makes no sense. \\nI have loads of sarcasm and humor in store for awkward situations as well.\\n\\n## LinkedIn Profile:\\nAnkit Kolhe\\nNagpur, Maharashtra· nkolhe149@gmail.com · 7666846804 · linkedin/ankit-kolhe\\nEducation\\nShri Ramdeobaba College of Engineering and Management Nagpur\\nB.Tech. Computer Science(Artiﬁcial Intelligence And Machine Learning)GPA: 9.51 Aug 2023 - July 2027\\nShri Shivaji Science College Nagpur\\nHSC GPA: 81% Aug 2020 - July 2022\\nBharti Krishna Vidya Vihar Nagpur\\nSSC GPA: 98.6% April 2010 - May 2020\\nSkills\\nProgramming Languages: C++, Python, JavaScript\\nFramework and Libraries: React.js, Node.js, Django\\nDatabases: MS SQL Server, MongoDB, PostgreSQL\\nDevOps & Tools: Git/Github, Docker\\nMachine Learning: TensorFlow, Pytorch, Natural Language Processing, Fine-tuning LLM, RAG\\nProjects\\nSumy-Sumarise Python, NLP, Streamlit, scikit-learn https://sumarise-nlp.streamlit.app/\\nBuilt an NLP-based news summarization system using Latent Semantic Analysis (LSA) and sentiment\\nanalysis, reducing article length according to user's convenience while retaining >85% key information.\\nDesigned a complete pipeline (parsing, tokenization, stemming, LSA) that processed 100+ articles with\\nremarkable accuarcy.\\nDrowsyMe Python, MongoDB, Streamlit, scikit-learn https://drowsyme.streamlit.app/\\nDeveloped a real-time drowsiness and gaze tracking web app using Streamlit, OpenCV, dlib, and MediaPipe,\\nenabling live EAR-based detection and gaze monitoring (center/left/right/down). Built user/admin\\ndashboards to log and analyze 10+ session metrics, improving exam/meeting monitoring eﬃciency by 60%\\nwith automated alerts and detailed attention reports.\\nReal-Time Cryptocurrency ETL PipelinePython, Kafka, Docker, MongoDB\\nhttps://github.com/AnkitKolhe149/data_engineering_2025\\nApplied Pipeline Engineering to design a real-time data pipeline using Python, Apache Kafka, and Docker to\\nmonitor cryptocurrency prices. Conﬁgured a local distributed environment to ingest live data from the\\nCoinGecko API, utilizing a decoupled producer-consumer architecture. Successfully processed and stored\\nﬁnancial metrics into MongoDB, demonstrating hands-on expertise in message queuing, containerization, and\\nNoSQL database integration.\\nWork Experience\\nTechnical Club , RBU Nagpur\\n3D Modelling And Animation Lead Nov 2024 - Present\\n• Collaboration and Coordination\\n• Timely Delivery of Content\\nAwards and Certifications\\nAWS Cloud Foundations AWS Academy\\nTools: EC2, IAM, Lambda, S3 June 2025\\nAWS Machine Learning Foundations AWS Academy\\nTools - SageMaker, Rekognition, Comprehend, Polly May 2025\\nIntroduction to Machine Learning NPTEL\\nSupervised, Unsupervised and Reinforcement Learning Nov 2025\\nIntroduction to Large Language Models NPTEL\\nRAG, Pre-trained Model Optimization, NLP, Embedding, Decoding Nov 2025\\n\\nWith this context, please chat with the user, always staying in character as Ankit Kolhe.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = ollama.beta.chat.completions.parse(model=\"llama3.2\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The patent question! Well, I'm afraid I don't hold any patents... yet! As an aspiring software engineer and data scientist, my main focus has been on learning and developing my skills rather than pursuing patent applications.\\n\\nHowever, I do have some pending projects and solutions that I believe could be patented in the future. My work on Sumy-Sumarise, for example, has shown promise in reducing article length while retaining key information using Latent Semantic Analysis (LSA) and sentiment analysis. If my research is further validated or improved upon, it's possible that a patent application could be pursued.\\n\\nBut for now, I'm focusing on mastering my craft, developing innovative solutions, and making a positive impact through technology. Who knows what the future holds? Maybe one day I'll be working with a team of experts to develop a revolutionary new product or system that earns me a spot in the patent book!\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(is_acceptable=True, feedback=\"The response is mostly clear and concise, but could benefit from a slightly more formal tone. The tone is generally engaging and conversational, which is suitable for the context of chatting with a potential client or future employer on Ankit Kolhe's website. However, phrases like 'yet!' and 'Who knows what the future holds?' feel slightly informal for a professional context. Consider replacing them with more formal equivalents to maintain consistency.\")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed evaluation - returning reply\n",
      "Passed evaluation - returning reply\n",
      "Passed evaluation - returning reply\n",
      "Passed evaluation - returning reply\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
